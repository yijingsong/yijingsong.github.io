<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="Tensorflow 2.0, yijingsong">
    <meta name="description" content="一、神经网络设计过程1.1 神经网络训练过程
采集大量数据对（输入特征，标签）构成数据集

将数据集喂入搭建好的神经网络结构

网络优化参数得到模型

模型读取新输入特征

输出识别结果


1.2 损失函数（loss function）
">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Tensorflow 2.0 | yijingsong</title>
    <link rel="icon" type="image/png" href="/favicon.png">
    


    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>
    
<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">yijingsong</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">yijingsong</div>
        <div class="logo-desc">
            
            疾风亦有归途
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/blinkfox/hexo-theme-matery" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/blinkfox/hexo-theme-matery" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/21.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Tensorflow 2.0</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <style>
    #toc-content {
        height: calc(100vh - 250px);
        overflow: scroll;
    }
</style>


<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                          <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                          </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-02-06
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2024-02-23
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    7.3k
                </div>
                

                

                
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.min.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h2 id="一、神经网络设计过程"><a href="#一、神经网络设计过程" class="headerlink" title="一、神经网络设计过程"></a>一、神经网络设计过程</h2><h3 id="1-1-神经网络训练过程"><a href="#1-1-神经网络训练过程" class="headerlink" title="1.1 神经网络训练过程"></a>1.1 神经网络训练过程</h3><ul>
<li><p>采集大量<strong>数据对</strong>（输入特征，标签）构成数据集</p>
</li>
<li><p>将数据集<strong>喂入</strong>搭建好的神经<strong>网络结构</strong></p>
</li>
<li><p>网络<strong>优化参数</strong>得到模型</p>
</li>
<li><p>模型读取新输入特征</p>
</li>
<li><p>输出识别结果</p>
</li>
</ul>
<h3 id="1-2-损失函数（loss-function）"><a href="#1-2-损失函数（loss-function）" class="headerlink" title="1.2 损失函数（loss function）"></a>1.2 损失函数（loss function）</h3><ul>
<li><strong>衡量预测值（y）与实际值（y_）的差距</strong></li>
<li>定量判断 权重$W$ 和 偏置$b$ 的优劣，当损失函数输出最小时，参数$W、b$ 会出现最优值</li>
</ul>
<h4 id="1-2-1-均方误差（MSE）"><a href="#1-2-1-均方误差（MSE）" class="headerlink" title="1.2.1 均方误差（MSE）"></a>1.2.1 均方误差（MSE）</h4><ul>
<li>$MSE(y_{-},y) = \frac{\sum^n_{k=0}(y-y_{-})^2}{n}$</li>
</ul>
<h3 id="1-3-梯度"><a href="#1-3-梯度" class="headerlink" title="1.3 梯度"></a>1.3 梯度</h3><ul>
<li>模型训练的目的是：找到一组参数$W、b$，使 $loss function$ 最小</li>
<li><strong>损失函数输出值减小的方向，即函数梯度下降的方向</strong><ul>
<li>梯度：函数对各参数求偏导后的向量</li>
</ul>
</li>
</ul>
<h4 id="1-3-1-梯度下降"><a href="#1-3-1-梯度下降" class="headerlink" title="1.3.1 梯度下降"></a>1.3.1 梯度下降</h4><ul>
<li>参数更新：<ul>
<li>$w_{t+1} = w_t - lr*\frac{\partial loss}{\partial w_t}$</li>
<li>$b_{t+1} = b_t - lr * \frac{\partial loss}{\partial b_t}$</li>
<li>$w_{t+1} * x + b_{t+1} \rightarrow y$</li>
</ul>
</li>
<li>上式中，$lr$ 为学习率（$learning$ $rate$），是梯度下降的速度，是一个超参数，当 $lr$ 设置过小时，收敛过程会变得十分缓慢，过大时，梯度可能会在最小值附近来回震荡，甚至无法收敛<img src="/2024/02/06/tensorflow-2-0/image-20240206125753088.png"></li>
</ul>
<h3 id="1-4-反向传播"><a href="#1-4-反向传播" class="headerlink" title="1.4 反向传播"></a>1.4 反向传播</h3><ul>
<li>从后向前，逐层求损失函数对每层神经元参数的偏导数，迭代更新所有参数</li>
</ul>
<h3 id="1-5-感受训练过程"><a href="#1-5-感受训练过程" class="headerlink" title="1.5 感受训练过程"></a>1.5 感受训练过程</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

# 设定可训练参数 w 的随机初始值为 5
w = tf.Variable(tf.constant(5, dtype = tf.float32))

# 学习率 lr 设置为 0.01
lr = 0.01

# 循环迭代次数 40次
epoch = 40

for epoch in range(epoch):
    with tf.GradientTape() as tape:
        # 损失函数 loss 定义为 (w + 1)^2
        loss = tf.square(w + 1)
    
    # .gradient()，指定损失函数 loss 关于 w 求导
    grads = tape.gradient(loss, w)
    
    # .assign_sub()，让变量做自减，即，w -= lr * grads
    w.assign_sub(lr * grads)
    print("After %s epoch, w is %f, loss is %f" % (epoch, w.numpy(), loss))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>



<h2 id="二、Tensorflow-2-0-——-基本知识"><a href="#二、Tensorflow-2-0-——-基本知识" class="headerlink" title="二、Tensorflow 2.0 —— 基本知识"></a>二、Tensorflow 2.0 —— 基本知识</h2><h3 id="2-1-创建-tensor"><a href="#2-1-创建-tensor" class="headerlink" title="2.1 创建 tensor"></a>2.1 创建 tensor</h3><ul>
<li>张量（$Tensor$）：<strong>多维数组</strong>（列表），可以表示0阶到n阶数组</li>
<li>阶：张量的维数</li>
</ul>
<table>
<thead>
<tr>
<th align="center">维数</th>
<th align="center">阶</th>
<th align="center">名字</th>
<th align="center">例子</th>
</tr>
</thead>
<tbody><tr>
<td align="center">0-D</td>
<td align="center">0</td>
<td align="center">标量 scalar</td>
<td align="center">s = 1</td>
</tr>
<tr>
<td align="center">1-D</td>
<td align="center">1</td>
<td align="center">向量 vector</td>
<td align="center">v = [1, 2, 3]</td>
</tr>
<tr>
<td align="center">2-D</td>
<td align="center">2</td>
<td align="center">矩阵 matrix</td>
<td align="center">m = [[1, 2, 3], [4, 5 6]]</td>
</tr>
<tr>
<td align="center">n-D</td>
<td align="center">n</td>
<td align="center">张量 tensor</td>
<td align="center">t = [[[[…]]]]</td>
</tr>
</tbody></table>
<h4 id="2-1-1-普通-tensor"><a href="#2-1-1-普通-tensor" class="headerlink" title="2.1.1 普通 tensor"></a>2.1.1 普通 tensor</h4><ul>
<li><code>tf.constant(张量内容, dtype = 数据类型（可选）)</code></li>
</ul>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

a = tf.constant([1, 5], dtype = tf.int64)

print(a)
print(a.dtype)
print(a.shape)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor([1 5], shape=(2,), dtype=int64)

&lt;dtype: 'int64'&gt;

(2,)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-1-2-numpy-转-tensor"><a href="#2-1-2-numpy-转-tensor" class="headerlink" title="2.1.2 numpy 转 tensor"></a>2.1.2 numpy 转 tensor</h4><ul>
<li><code>tf.convert_to_tensor(数据名, dtype = 数据类型（可选）)</code></li>
</ul>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf
import numpy as np

a = np.arange(0, 5)
b = tf.convert_to_tensor(a, dtype = tf.int64)

print(a)
print(b)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">[0 1 2 3 4]

tf.Tensor([0 1 2 3 4], shape=(5,), dtype=int64)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-1-3-同值-tensor"><a href="#2-1-3-同值-tensor" class="headerlink" title="2.1.3 同值 tensor"></a>2.1.3 同值 tensor</h4><ul>
<li><p><code>tf.zeros(维度)</code>：全 $0$ $tensor$</p>
</li>
<li><p><code>tf.ones(维度)</code>：全 $1$ $tensor$</p>
</li>
<li><p><code>tf.fill(维度, 指定值)</code>：全为指定值的 $tensor$</p>
</li>
</ul>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

a = tf.zeros([2, 3])
b = tf.ones(4)
c = tf.fill([2, 2], 9)

print(a)
print(b)
print(c)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor(
[[0. 0. 0.]
 [0. 0. 0.]], shape=(2, 3), dtype=float32)
 
tf.Tensor([1. 1. 1. 1.], shape=(4,), dtype=float32)

tf.Tensor(
[[9 9]
 [9 9]], shape=(2, 2), dtype=int32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-1-4-统计分布-tensor"><a href="#2-1-4-统计分布-tensor" class="headerlink" title="2.1.4 统计分布 tensor"></a>2.1.4 统计分布 tensor</h4><ul>
<li><p><code>tf.random.normal(维度, mean = 均值, stddev = 标准差)</code>：正太分布随机数，默认均值0，标准差1</p>
</li>
<li><p><code>tf.random.truncated_normal(维度, mean = 均值, stddev = 标准差)</code>：生成$(\mu-2\sigma, \mu+2\sigma)$之间的正态分布值，若生成值在$(\mu-2\sigma, \mu+2 \sigma)$之外，会进行重新生成，保证了取值在均值附近</p>
</li>
<li><p>标准差计算公式：$\sigma = \sqrt{\frac{\sum^n_{i = 1}(x_i - \bar x)^2}{n}}$</p>
</li>
<li><p><code>tf.random.uniform(维度, minval = 最小值, maxval = 最大值)</code>：均匀分布随机数</p>
</li>
</ul>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

d = tf.random.normal([2, 2], mean = 0.5, stddev = 1)
print(d)

e = tf.random.truncated_normal([2, 2], mean = 0.5, stddev = 1)
print(e)

f = tf.random.uniform([2, 2], minval = 0, maxval = 1)
print(f)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor(
[[-1.7673643   1.8214508 ]
 [ 1.3998327  -0.46976972]], shape=(2, 2), dtype=float32)
 
tf.Tensor(
[[ 0.9264284  -0.69317317]
 [ 2.2075253   1.5080855 ]], shape=(2, 2), dtype=float32)
 
 tf.Tensor(
[[0.7370062  0.24481273]
 [0.06990469 0.4966843 ]], shape=(2, 2), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h3 id="2-2-常用函数"><a href="#2-2-常用函数" class="headerlink" title="2.2 常用函数"></a>2.2 常用函数</h3><h4 id="2-2-1-类型强转-极值"><a href="#2-2-1-类型强转-极值" class="headerlink" title="2.2.1 类型强转 &amp; 极值"></a>2.2.1 类型强转 &amp; 极值</h4><ul>
<li><code>tf.cast(张量名, dtype = 数据类型)</code>：强制类型转换</li>
<li><code>tf.reduce_min(张量名)</code>：计算张量维度上元素的最小值</li>
<li><code>tf.reduce_max(张量名)</code>：计算张量维度上的元素最大值</li>
</ul>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

x1 = tf.constant([1., 2., 3.], dtype = tf.float64)
print(x1)

x2 = tf.cast(x1, tf.int32)
print(x2)

print(tf.reduce_min(x2), tf.reduce_max(x2))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor([1. 2. 3.], shape=(3,), dtype=float64)

tf.Tensor([1 2 3], shape=(3,), dtype=int32)

tf.Tensor(1, shape=(), dtype=int32) tf.Tensor(3, shape=(), dtype=int32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-2-参数-axis"><a href="#2-2-2-参数-axis" class="headerlink" title="2.2.2 参数 axis"></a>2.2.2 参数 axis</h4><ul>
<li><p>在一个二维 $tensor$ 或数组中，可以通过调整 $axis$ 等于 $0$ 或 $1$ 控制执行维度</p>
<ul>
<li><p>$axis = 0$ ：代表跨行（经度，down）</p>
</li>
<li><p>$axis = 1$ ：代表跨列（纬度，across）</p>
</li>
<li><p>若不指定 $axis$ ，则所有元素参与计算</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240207111416067.png"></p>
<ul>
<li><code>tf.reduce_mean(张量名, axis = 操作轴)</code>：计算 $tensor$ 沿指定维度的平均值</li>
<li><code>tf.reduce_sum(张量名, axis = 操作轴)</code>：计算 $tensor$ 沿指定维度的和</li>
</ul>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

x = tf.constant([[1, 2, 3], [1, 2, 3]])

print(x)
print(tf.reduce_mean(x))
print(tf.reduce_sum(x, axis = 1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor(
[[1 2 3]
 [1 2 3]], shape=(2, 3), dtype=int32)
 
tf.Tensor(2, shape=(), dtype=int32)

tf.Tensor([6 6], shape=(2,), dtype=int32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<ul>
<li><code>tf.argmax(张量名, axis = 操作轴)</code>：返回张量沿指定维度最大值的索引</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf
import numpy as np

test = np.array([[1, 2, 3], [2, 3, 4], [5, 4, 3], [8, 7, 2]])

print(test)
print(tf.argmax(test, axis = 0)) # 返回每一列最大值的索引
print(tf.argmax(test, axis = 1)) # 返回每一行最大值的索引<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">[[1 2 3]
 [2 3 4]
 [5 4 3]
 [8 7 2]]
 
tf.Tensor([3 3 1], shape=(3,), dtype=int64)

tf.Tensor([2 2 0 0], shape=(4,), dtype=int64)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-3-变量标记“可训练”"><a href="#2-2-3-变量标记“可训练”" class="headerlink" title="2.2.3 变量标记“可训练”"></a>2.2.3 变量标记“可训练”</h4><ul>
<li><code>tf.Variable(初始值)</code>：将变量标记为“可训练”，被标记的变量会在<strong>反向传播中记录梯度信息</strong>。神经网络训练中，常用该函数<strong>标记待训练参数</strong></li>
</ul>
<p>例如：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">'''
注释：
	首先生成正太分布随机数，
	再给生成的随机数标记为可训练，
	之后在反向传播中就可以通过梯度下降更新参数 w 了
'''

w = tf.Variable(tf.random.normal([2, 2], mean = 0, stddev = 1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-2-4-数学运算"><a href="#2-2-4-数学运算" class="headerlink" title="2.2.4 数学运算"></a>2.2.4 数学运算</h4><ul>
<li><code>tf.add(张量1, 张量2)</code>、<code>tf.subtract(张量1, 张量2)</code>、<code>tf.multiply(张量1, 张量2)</code>、<code>tf.divide(张量1, 张量2)</code>：<strong>对应元素</strong>四则运算，只有<strong>维度相同</strong>的张量才可以做四则运算</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

a = tf.ones([1, 3])
b = tf.fill([1, 3], 3.)

print(a)
print(b)

print(tf.add(a, b))
print(tf.subtract(a, b))
print(tf.multiply(a, b))
print(tf.divide(a, b))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor([[1. 1. 1.]], shape=(1, 3), dtype=float32)

tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)


tf.Tensor([[4. 4. 4.]], shape=(1, 3), dtype=float32)

tf.Tensor([[-2. -2. -2.]], shape=(1, 3), dtype=float32)

tf.Tensor([[3. 3. 3.]], shape=(1, 3), dtype=float32)

tf.Tensor([[0.33333334 0.33333334 0.33333334]], shape=(1, 3), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<ul>
<li><code>tf.square</code>(张量名)、<code>tf.pow(张量名, n次方数)</code>、<code>tf.sqrt(张量名)</code>：平方、次方、开方<ul>
<li>其中，需要注意的是<code>tf.sqrt()</code><strong>不能对整型数据进行开方</strong>，只能对浮点型数据进行运算</li>
</ul>
</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

a = tf.fill([1, 2], 3.)

print(a)

print(tf.pow(a, 3))

print(tf.square(a))

print(tf.sqrt(a))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor([[3. 3.]], shape=(1, 2), dtype=float32)
tf.Tensor([[27. 27.]], shape=(1, 2), dtype=float32)
tf.Tensor([[9. 9.]], shape=(1, 2), dtype=float32)
tf.Tensor([[1.7320508 1.7320508]], shape=(1, 2), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<ul>
<li><code>tf.matmul(矩阵1, 矩阵2)</code>：矩阵乘</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

a = tf.ones([3, 2])
b = tf.fill([2, 3], 3.)

print(tf.matmul(a, b))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor(
[[6. 6. 6.]
 [6. 6. 6.]
 [6. 6. 6.]], shape=(3, 3), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-5-构建数据集"><a href="#2-2-5-构建数据集" class="headerlink" title="2.2.5 构建数据集"></a>2.2.5 构建数据集</h4><ul>
<li><code>tf.data.Dataset.from_tensor_slices((输入特征, 标签))</code>：切分传入张量的第一维度，<strong>生成输入特征/标签对</strong>，构建数据集（$numpy$ 和 $tensor$ 格式都可用该读入数据）</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

features = tf.constant([12, 23, 10, 17])
labels = tf.constant([0, 1, 1, 0])
dataset = tf.data.Dataset.from_tensor_slices((features, labels))

print(dataset)

for element in dataset:
    print(element)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">&lt;TensorSliceDataset shapes: ((), ()), types: (tf.int32, tf.int32)&gt;
(&lt;tf.Tensor: shape=(), dtype=int32, numpy=12&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;)
(&lt;tf.Tensor: shape=(), dtype=int32, numpy=23&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;)
(&lt;tf.Tensor: shape=(), dtype=int32, numpy=10&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;)
(&lt;tf.Tensor: shape=(), dtype=int32, numpy=17&gt;, &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-6-求导运算"><a href="#2-2-6-求导运算" class="headerlink" title="2.2.6 求导运算"></a>2.2.6 求导运算</h4><ul>
<li><code>tf.GradientTape()</code></li>
<li>一般使用 $with$ 结构记录计算过程，$gradient$ 求出张量的梯度</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">with tf.GradientTape() as tape:
    # 若干计算过程
grad = tape.gradient(函数, 求导变量)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>举例：</p>
<p>计算 $\frac{\partial w^2}{\partial w} = 2w$ ，当 $w = 3.0$ 时的导数值</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

with tf.GradientTape() as tape:
    w = tf.Variable(tf.constant(3.0))
    loss = tf.pow(w, 2)

grad = tape.gradient(loss, w)
print(grad)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor(6.0, shape=(), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-7-枚举"><a href="#2-2-7-枚举" class="headerlink" title="2.2.7 枚举"></a>2.2.7 枚举</h4><ul>
<li><code>enumerate(列表名)</code>：$python$ 内建函数，可遍历每个元素（如列表、元组或字符串），组合为：索引 + 元素</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">seq = ['one', 'two', 'three']
for i, element in enumerate(seq):
    print(i, element)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">0 one
1 two
2 three<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-8-独热编码-one-hot-encoding"><a href="#2-2-8-独热编码-one-hot-encoding" class="headerlink" title="2.2.8 独热编码 (one-hot encoding)"></a>2.2.8 独热编码 (one-hot encoding)</h4><ul>
<li><p><code>tf.one_hot(待转换数据, depth = 几分类)</code>：将待转换数据转换为 $one-hot$ 形式的数据输出。分类问题中，常用独热编码做标签，标记类别 <strong>$1$ 表示是，$0$ 表示非</strong>，编码的<strong>最小标签值是 $0$</strong></p>
</li>
<li><p>编码规则：以鸢尾花分类为例，以 $0$ 代表狗尾草鸢尾，$1$ 表示杂色鸢尾，$2$ 表示弗吉尼亚鸢尾</p>
<ul>
<li>对于标签 $1$</li>
<li>独热码：$(0. \quad 1. \quad 0.)$ ，表示非狗尾草鸢尾、是杂色鸢尾、非弗吉尼亚鸢尾</li>
</ul>
</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

classes = 3
labels = tf.constant([1, 0, 2]) # 最小值为0，最大值为2
output = tf.one_hot(labels, depth = classes)

print(output)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor(
[[0. 1. 0.]
 [1. 0. 0.]
 [0. 0. 1.]], shape=(3, 3), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-9-得分概率化"><a href="#2-2-9-得分概率化" class="headerlink" title="2.2.9 得分概率化"></a>2.2.9 得分概率化</h4><ul>
<li><code>tf.nn.softmax(输出的各类得分)</code>：当 $n$ 分类的 $n$ 个输出 $(y_0,y_1,…,y_{n-1})$ 通过 $softmax()$ 函数，使得概率化，且<strong>各类得分概率化后的总和为 $1$<strong>。通过神经网络权重 $w$ 和偏置 $b$ 计算得到的每种类型的得分，这些数字只有</strong>符合概率分布后，才可以与独热码的标签作比较</strong><ul>
<li>使用公式$\frac{e^{y_i}}{\sum^n_{j = 0} e^{y_j}}$，可将输出的得概率化</li>
</ul>
</li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240207150745130.png" alt="网络计算输出得分"></p>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240207150934015.png" alt="得分概率化"></p>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

y = tf.constant([1.01, 2.01, -0.66]) # 神经网络前向传播后的结果
y_pro = tf.nn.softmax(y)

print(y_pro)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">tf.Tensor([0.25598174 0.69583046 0.04818781], shape=(3,), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</blockquote>
<h4 id="2-2-10-参数自更新"><a href="#2-2-10-参数自更新" class="headerlink" title="2.2.10 参数自更新"></a>2.2.10 参数自更新</h4><ul>
<li><code>可训练变量.assign_sub(变量自减的内容)</code>：自减运算，更新参数值并返回。调用 $assign_sub$ 前，先用 $tf.Variable()$ 定义变量为可训练（可自更新）</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

w = tf.Variable(4)
w.assign_sub(1)

print(w)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">&lt;tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</blockquote>
<h3 id="2-3-案例-——-鸢尾花分类"><a href="#2-3-案例-——-鸢尾花分类" class="headerlink" title="2.3. 案例 —— 鸢尾花分类"></a>2.3. 案例 —— 鸢尾花分类</h3><h4 id="2-3-1-Iris-数据集"><a href="#2-3-1-Iris-数据集" class="headerlink" title="2.3.1 Iris 数据集"></a>2.3.1 Iris 数据集</h4><ul>
<li>$Iris$ 数据集共有 $150$ 组，每组包括花萼长、花萼宽、花瓣长、花瓣宽 $4$ 个输入特征。同时，给出了每组特征对应的鸢尾花类别，类别包括狗尾草鸢尾花、杂色鸢尾花、弗吉尼亚鸢尾花 $3$ 类，分别用数字 $0,1,2$ 表示</li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240207153922533.png"></p>
<ul>
<li>从 $sklearn$ 包 $datasets$ 读入数据集</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn.datasets import load_iris

x_data = load_iris().data   # 返回 iris 数据集所有输入特征
y_data = load_iris().target # 返回 iris 数据集所有标签<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<ul>
<li>另外，可将 $Iris$ 数据集通过 $pandas$ 库绘制表格进行可视化</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn import datasets
from pandas import DataFrame
import pandas as pd

x_data = datasets.load_iris().data
y_data = datasets.load_iris().target
print("x_data from datasets：\n", x_data)
print("y_data from datasets：\n", y_data)

x_data = DataFrame(x_data, columns = ['花萼长', '花萼宽', '花瓣长', '花瓣宽'])
pd.set_option('display.unicode.east_asian_width', True) # 设置列名对齐
print("x_data add index：\n", x_data)

x_data['类别'] = y_data # 新加一列，列名为 类别，数据为 y_data
print("x_data add a column：\n", x_data)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-3-2-实现过程概览"><a href="#2-3-2-实现过程概览" class="headerlink" title="2.3.2 实现过程概览"></a>2.3.2 实现过程概览</h4><ul>
<li><p>准备数据集</p>
<ul>
<li>数据集读入</li>
<li>数据集乱序</li>
<li>生成训练集和测试集 $(x_train / y_{-}train,\quad x_test/y_{-}test)$</li>
<li>配对（输入特征，标签），每次读入一小撮 $(batch)$</li>
</ul>
</li>
<li><p>搭建网络</p>
<ul>
<li>定义神经网络中所有可训练参数</li>
</ul>
</li>
<li><p>参数优化</p>
<ul>
<li>嵌套循环迭代，$with$ 结构更新参数，显示当前 $loss$</li>
</ul>
</li>
<li><p>测试效果</p>
<ul>
<li>计算当前参数前向传播后的准确率，显示当前 $acc$</li>
</ul>
</li>
<li><p>$acc/loss$ 可视化</p>
</li>
</ul>
<h4 id="2-3-3-准备数据集"><a href="#2-3-3-准备数据集" class="headerlink" title="2.3.3 准备数据集"></a>2.3.3 准备数据集</h4><h5 id="2-3-3-1-数据集读入"><a href="#2-3-3-1-数据集读入" class="headerlink" title="2.3.3.1 数据集读入"></a>2.3.3.1 数据集读入</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python">from sklearn import datasets
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h5 id="2-3-3-2-数据集乱序"><a href="#2-3-3-2-数据集乱序" class="headerlink" title="2.3.3.2 数据集乱序"></a>2.3.3.2 数据集乱序</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python">np.random.seed(116) # 使用相同的种子，使 特征/标签 一一对应
np.random.shuffle(x_data)

np.random.seed(116)
np.random.shuffle(y_data)

tf.random.set_seed(116)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="2-3-3-3-划分-train-test"><a href="#2-3-3-3-划分-train-test" class="headerlink" title="2.3.3.3 划分 train &amp; test"></a>2.3.3.3 划分 train &amp; test</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python">x_train = x_data[:-30]
y_train = y_data[:-30]

x_test = x_data[-30:]
y_test = y_data[-30:]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h5 id="2-3-3-4-特征-标签-配对"><a href="#2-3-3-4-特征-标签-配对" class="headerlink" title="2.3.3.4 特征/标签 配对"></a>2.3.3.4 特征/标签 配对</h5><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 每 32 组标签对，打包为一个 batch
# 喂入神经网络的数据，是以 batch 为单位
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-3-4-搭建网络"><a href="#2-3-4-搭建网络" class="headerlink" title="2.3.4 搭建网络"></a>2.3.4 搭建网络</h4><ul>
<li>定义神经网络中的所有可训练参数</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev = 0.1, seed = 1))

b1 = tf.Variable(tf.random.truncated_normal([3], stddev = 0.1, seed = 1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<h4 id="2-3-5-参数优化"><a href="#2-3-5-参数优化" class="headerlink" title="2.3.5 参数优化"></a>2.3.5 参数优化</h4><ul>
<li>嵌套循环迭代，$with$ 结构更新参数，显示当前 $loss$</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for epoch in range(epoch):
    for step, (x_train, y_train) in enumerate(train_db): # batch 级别迭代
        with tf.GradientTape() as tape: # 记录梯度信息
            # 前向传播过程计算 y
            # 计算总 loss
        grads = tape.gradient(loss, [w1, b1])
        w1.assign_sub(lr * grads[0])
        b1.assign_sub(lr * grads[1])
    print("Epoch: {}, loss: {}".format(epoch, loss_all/4))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-3-6-测试效果"><a href="#2-3-6-测试效果" class="headerlink" title="2.3.6 测试效果"></a>2.3.6 测试效果</h4><ul>
<li>计算当前参数前向传播后的准确率，显示当前acc</li>
</ul>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">for x_test, y_test in test_db:
    y = tf.matmul(x_train, w1) + b1 # y 为预测结果
    y = tf.nn.softmax(y)    # y 概率化
    
    pred = tf.argmax(y, axis = 1) # 返回 y 中最大值的索引
    pred = tf.cast(pred, dtype = y_test.dtype) # 调整数据类型与标签一致
    
    correct = tf.cast(tf.equal(pred, y_test), dtype = tf.int32)
    correct = tf.reduce_sum(correct) # 将每个 batch 中的 correct 数加起来
    
    total_correct += int(correct) # 将所有 batch 中的 correct 数加起来
    total_number += x_test.shape[0]
    
acc = total_correct / total_number
print("test_acc: ", acc)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-3-7-acc-loss-可视化"><a href="#2-3-7-acc-loss-可视化" class="headerlink" title="2.3.7 acc/loss 可视化"></a>2.3.7 acc/loss 可视化</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python">plt.title('Acc Curve')
plt.xlabel('Epoch')
plt.ylabel('Acc')
plt.plot(test_acc, label = "$Accuracy$")
plt.legend()
plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="2-3-8-完整代码"><a href="#2-3-8-完整代码" class="headerlink" title="2.3.8 完整代码"></a>2.3.8 完整代码</h4><pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入模块
import tensorflow as tf
from sklearn import datasets
from matplotlib import pyplot as plt
import numpy as np<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 导入数据
x_data = datasets.load_iris().data
y_data = datasets.load_iris().target<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 数据集乱序
np.random.seed(116) # 使用相同的种子，使 特征/标签 一一对应
np.random.shuffle(x_data)

np.random.seed(116)
np.random.shuffle(y_data)

tf.random.set_seed(116)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 划分 train 和 test
x_train = x_data[:-30]
y_train = y_data[:-30]

x_test = x_data[-30:]
y_test = y_data[-30:]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 转换 x 的数据类型
# 避免之后矩阵相乘时，因数据类型不一致而报错
x_train= tf.cast(x_train, tf.float32)
x_test = tf.cast(x_test, tf.float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 特征/标签 配对
# 把数据集分批次，每个批次batch组数据
train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)

test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 定义神经网络参数
w1 = tf.Variable(tf.random.truncated_normal([4, 3], stddev = 0.1, seed = 1))

b1 = tf.Variable(tf.random.truncated_normal([3], stddev = 0.1, seed = 1))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 定义超参数及画图要用的acc/loss列表
lr = 0.1 # 学习率
epoch = 500 # 循环500轮

train_loss_results = [] # 将每轮的loss记录到此列表中，为后续画loss曲线提供数据
test_acc = [] # 将每轮的acc记录在此列表中，为后续画acc曲线提供数据

loss_all = 0 # 每轮分为4个step，loss_all记录4个step生成的4个loss的和<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 参数优化
for epoch in range(epoch):
    for step, (x_train, y_train) in enumerate(train_db): # batch 级别迭代
        with tf.GradientTape() as tape: # 记录梯度信息
            # 前向传播过程计算 y
            y = tf.matmul(x_train, w1) + b1
            y = tf.nn.softmax(y)
            y_ = tf.one_hot(y_train, depth = 3)
            
            # 计算总 loss
            loss = tf.reduce_mean(tf.square(y_ - y))
            loss_all += loss.numpy()
        
        # loss 对 w1 和 b1 求偏导
        grads = tape.gradient(loss, [w1, b1])
        
        # 参数更新
        w1.assign_sub(lr * grads[0])
        b1.assign_sub(lr * grads[1])
    
    #每个 epoch 打印 loss 信息
    print("Epoch: {}, loss: {}".format(epoch, loss_all / 4))
    train_loss_results.append(loss_all / 4) # 记录每个epoch的平均loss
    loss_all = 0 # 为下一次epoch的loss做准备
    
    
    # 测试效果
    total_correct, total_number = 0, 0 # 两个变量分别为预测对的样本个数和测试的总样本数

    for x_test, y_test in test_db:
        # 使用更新后的参数进行预测
        y = tf.matmul(x_test, w1) + b1 # y 为预测结果
        y = tf.nn.softmax(y)    # y 概率化

        pred = tf.argmax(y, axis = 1) # 返回 y 中最大值的索引
        pred = tf.cast(pred, dtype = y_test.dtype) # 调整数据类型与标签一致

        correct = tf.cast(tf.equal(pred, y_test), dtype = tf.int32)
        correct = tf.reduce_sum(correct) # 将每个 batch 中的 correct 数加起来

        total_correct += int(correct) # 将所有 batch 中的 correct 数加起来
        total_number += x_test.shape[0]

    acc = total_correct / total_number
    test_acc.append(acc)
    print("test_acc: ", acc)
    print("--------------------------")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 绘制loss曲线
plt.title('Loss Function Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(train_loss_results, label = "$Loss$")
plt.legend()
plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<pre class="line-numbers language-python" data-language="python"><code class="language-python"># 绘制acc曲线
plt.title('Acc Curve')
plt.xlabel('Epoch')
plt.ylabel('Acc')
plt.plot(test_acc, label = "$Accuracy$")
plt.legend()
plt.show()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240207173134638.png"></p>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240207173233205.png"></p>
<h2 id="三、Tensorflow-2-0-——-网络优化"><a href="#三、Tensorflow-2-0-——-网络优化" class="headerlink" title="三、Tensorflow 2.0 —— 网络优化"></a>三、Tensorflow 2.0 —— 网络优化</h2><ul>
<li>使用<strong>正则化</strong>减少过拟合，使用<strong>优化器</strong>更新网络参数</li>
</ul>
<h3 id="3-1-一些函数"><a href="#3-1-一些函数" class="headerlink" title="3.1 一些函数"></a>3.1 一些函数</h3><h4 id="3-1-1-条件语句"><a href="#3-1-1-条件语句" class="headerlink" title="3.1.1 条件语句"></a>3.1.1 条件语句</h4><ul>
<li><code>tf.where(条件语句, 真返回A, 假返回B)</code></li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

a = tf.constant([1, 2, 3, 1, 1])
b = tf.constant([0, 1, 3, 4, 5])

# tf.greater()，将两个 tensor 的对应元素进行比较
c = tf.where(tf.greater(a, b), a, b)
print("c: ", c)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">c:  tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
</blockquote>
<h4 id="3-1-2-0-1-的随机数"><a href="#3-1-2-0-1-的随机数" class="headerlink" title="3.1.2 [0, 1) 的随机数"></a>3.1.2 [0, 1) 的随机数</h4><ul>
<li><code>tf.random.RandomState.rand(维度)</code>：返回一个 $[0, 1)$ 的随机数，若维度为空，则返回标量</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf
import numpy as np
rdm = np.random.RandomState(seed = 1) # seed = 常数，使每次生成的随机数相同
a = rdm.rand() # 标量
b = rdm.rand(2, 3) # 2行3列，维度不能加 [] ，会报错

print(a)
print(b)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">0.417022004702574

[[7.20324493e-01 1.14374817e-04 3.02332573e-01]
 [1.46755891e-01 9.23385948e-02 1.86260211e-01]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="3-1-3-生成网格坐标点"><a href="#3-1-3-生成网格坐标点" class="headerlink" title="3.1.3 生成网格坐标点"></a>3.1.3 生成网格坐标点</h4><ul>
<li><code>np.mgrid[起始值 : 结束值 : 步长, 起始值 : 结束值 : 步长]</code>：[起始值, 结束值)</li>
<li><code>变量.ravel()</code>：将变量一维化，变为一维数组</li>
<li><code>np.c_[数组1, 数组2, ...]</code>：使返回的间隔数值点配对</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf
import numpy as np

x, y = np.mgrid[1:3:1, 2:4:0.5]
grid = np.c_[x.ravel(), y.ravel()]

print(x)
print(y)
print(grid)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">[[1. 1. 1. 1.]
 [2. 2. 2. 2.]]
 
[[2.  2.5 3.  3.5]
 [2.  2.5 3.  3.5]]
 
[[1.  2. ]
 [1.  2.5]
 [1.  3. ]
 [1.  3.5]
 [2.  2. ]
 [2.  2.5]
 [2.  3. ]
 [2.  3.5]]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h3 id="3-2-复杂度-学习率"><a href="#3-2-复杂度-学习率" class="headerlink" title="3.2 复杂度 &amp; 学习率"></a>3.2 复杂度 &amp; 学习率</h3><h4 id="3-2-1-复杂度"><a href="#3-2-1-复杂度" class="headerlink" title="3.2.1 复杂度"></a>3.2.1 复杂度</h4><ul>
<li>复杂度分为<strong>空间复杂度</strong>和<strong>时间复杂度</strong></li>
<li>神经网络复杂度，常用 $NN$ 层数和 $NN$ 参数个数表示</li>
</ul>
<h5 id="3-2-1-1-空间复杂度"><a href="#3-2-1-1-空间复杂度" class="headerlink" title="3.2.1.1 空间复杂度"></a>3.2.1.1 空间复杂度</h5><ul>
<li>空间复杂度有<strong>层数</strong>和<strong>总参数个数</strong>两个指标<ul>
<li>$层数 = 隐藏层层数 + 1个输出层$</li>
<li>$总参数个数 = 总 w + 总b$ ，注意 $b$ ，每个神经元只有一个偏置，但是 $w$ 跟前一层神经元连接状况相关</li>
</ul>
</li>
</ul>
<h5 id="3-2-1-2-时间复杂度"><a href="#3-2-1-2-时间复杂度" class="headerlink" title="3.2.1.2 时间复杂度"></a>3.2.1.2 时间复杂度</h5><ul>
<li>乘加运算次数，整个网络各神经元需要计算的总次数</li>
</ul>
<h4 id="3-2-2-学习率"><a href="#3-2-2-学习率" class="headerlink" title="3.2.2 学习率"></a>3.2.2 学习率</h4><ul>
<li><p>参数更新计算公式</p>
<ul>
<li>$w_{t+1} = w_t - lr*\frac{\partial loss}{\partial w_t}$</li>
<li>$b_{t+1} = b_t - lr * \frac{\partial loss}{\partial b_t}$</li>
</ul>
</li>
<li><p>学习率 $lr$ 设置国小，收敛速度很慢，过大可能不收敛</p>
</li>
<li><p>可以先使用较大的学习率，快速得到较优解，然后逐步减小学习率，使模型在训练后期稳定</p>
</li>
</ul>
<h5 id="3-2-2-1-指数衰减学习率"><a href="#3-2-2-1-指数衰减学习率" class="headerlink" title="3.2.2.1 指数衰减学习率"></a>3.2.2.1 指数衰减学习率</h5><ul>
<li>$指数衰减学习率 = 初始学习率 * 学习率衰减率^{\frac{当前轮数}{多少轮衰减一次}}$</li>
<li>通过该公式动态改变学习率的值，其中，初始学习率、学习率衰减率、多少轮衰减一次，为超参数</li>
</ul>
<p>代码示例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">epoch = 40
LR_BASE = 0.2
LR_DECAY = 0.99
LR_STEP = 1

for epoch in range(epoch):
    lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP)
    with tf.GradientTape() as tape:
        loss = tf.square(w + 1)
    grads = tape.gradient(loss, w)
    
    w.assign_sub(lr * grads)
    print("After %s epoch, w is %f, loss is %f, lr is %f" %(epoch, w.numpy(), loss, lr))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h3 id="3-3-激活函数"><a href="#3-3-激活函数" class="headerlink" title="3.3 激活函数"></a>3.3 激活函数</h3><h4 id="3-3-1-Sigmoid-函数"><a href="#3-3-1-Sigmoid-函数" class="headerlink" title="3.3.1 Sigmoid 函数"></a>3.3.1 Sigmoid 函数</h4><ul>
<li><p><code>tf.nn.sigmoid(x)</code></p>
</li>
<li><p>$f(x)=\frac{1}{1+e^{-x}}$</p>
</li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240220170857080.png"></p>
<ul>
<li>特点<ul>
<li><strong>易造成梯度消失</strong>，深层神经网络更新参数时，需要从输出层到输入层，逐层进行链式求导，$Sigmoid$ 函数导数取值是 $0$ - $0.25$ 之间的小数，而链式求导需要多层导数连续相乘，会出现多个 $0$ - $0.25$ 之间的小数相乘，结果将趋于 $0，产生梯度消失，使得参数无法继续更新</li>
<li><strong>输出非$0$均值，收敛慢</strong>，经过 $Sigmoid$ 函数后的数据都是正数，导致收敛速度慢</li>
<li><strong>幂运算复杂，训练时间长</strong>，$Sigmoid$ 函数本身存在幂运算，计算复杂，导致训练时间长</li>
</ul>
</li>
</ul>
<h4 id="3-3-2-Tanh-函数"><a href="#3-3-2-Tanh-函数" class="headerlink" title="3.3.2 Tanh 函数"></a>3.3.2 Tanh 函数</h4><ul>
<li><p><code>tf.math.tanh(x)</code></p>
</li>
<li><p>$f(x)=\frac{1-e^{-2x}}{1+e^{-2x}}$</p>
</li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240220172334130.png"></p>
<ul>
<li>特点<ul>
<li><strong>易造成梯度消失</strong></li>
<li><strong>输出是 $0$ 均值</strong></li>
<li><strong>幂运算复杂，训练时间长</strong></li>
</ul>
</li>
</ul>
<h4 id="3-3-3-Relu-函数"><a href="#3-3-3-Relu-函数" class="headerlink" title="3.3.3 Relu 函数"></a>3.3.3 Relu 函数</h4><ul>
<li><p><code>tf.nn.relu(x)</code></p>
</li>
<li><p>$ f(x)=max(x, 0) = \begin{cases}<br>0 &amp; x &lt; 0 \newline<br>x &amp; x &gt;= 0<br>\end{cases}$</p>
</li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240220173213855.png"></p>
<ul>
<li>优点<ul>
<li><strong>在正区间，解决了梯度消失问题</strong></li>
<li><strong>只需判断输入是否大于$0$，计算速度快</strong></li>
<li><strong>收敛速度远快于 $Sigmoid$ 和 $Tanh$</strong></li>
</ul>
</li>
<li>缺点<ul>
<li><strong>输出非 $0$ 均值，收敛慢</strong></li>
<li><strong>$Dead$ $Relu$ 问题</strong>：某些神经元可能永远不会被激活，导致相应的参数永远不能被更新。当输入是负数时，$Relu$ 函数输出为负数，反向传播更新参数时，梯度取值为$0$，导致参数无法更新，造成神经元死亡。造成神经元死亡的根本原因是，输入 $Relu$ 函数的负数特征过多，可以改进随机初值化，避免过多的负数特征送入 $Relu$ 函数，可以通过设置更小的学习率 $lr$ ，减小参数分布的巨大变化，避免训练中产生过多负数特征进入 $Relu$ 函数</li>
</ul>
</li>
</ul>
<h4 id="3-3-4-Leaky-Relu-函数"><a href="#3-3-4-Leaky-Relu-函数" class="headerlink" title="3.3.4 Leaky Relu 函数"></a>3.3.4 Leaky Relu 函数</h4><ul>
<li><code>tf.nn.leaky_relu(x)</code></li>
<li>$f(x)=max(\alpha x, x)$</li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240220174441482.png"></p>
<ul>
<li>$Leaky$ $Relu$ 函数是为了解决 $Relu$ 函数负区间为 $0$，引起神经元死亡的问题而设计的。$Leaky$ $Relu$函数在负区间引入斜率 $\alpha$ ，使负区间的取值不为 $0$</li>
<li>理论上，$Leaky$ $Relu$ 函数有 $Relu$ 函数的所有优点，且不会有 $Dead$ $ Relu$ 问题，但是在实际操作中，并没有完全证明 $Leaky$ $Relu$ 函数总是好于 $Relu$ 函数</li>
<li>在实际中，选择 $Relu$ 做激活函数的网络更多</li>
</ul>
<h4 id="3-3-5-一些建议"><a href="#3-3-5-一些建议" class="headerlink" title="3.3.5 一些建议"></a>3.3.5 一些建议</h4><ul>
<li><strong>首选 $Relu$</strong> 做激活函数</li>
<li><strong>学习率设置较小值</strong></li>
<li><strong>输入特征标准化</strong>，即让输入特征满足以 $0$ 为均值，$1$ 为标准差的正态分布</li>
<li><strong>初始参数中心化</strong>，即让随机生成的参数满足以 $0$ 为均值，$\sqrt{\frac{2}{当前层输入特征个数}}$ 为标准差的正态分布</li>
</ul>
<h3 id="3-4-损失函数"><a href="#3-4-损失函数" class="headerlink" title="3.4 损失函数"></a>3.4 损失函数</h3><ul>
<li>损失函数 ($loss$) ：是预测值 ($y$) 与已知答案 ($y_{-}$​) 的差距</li>
<li>神经网络的优化目标，就是找到一组参数，使得计算得到的 $loss$ 最小</li>
<li>主流 $loss$ 有 $3$ 种计算方法 $\begin{cases}<br>mse(Mean\ Squared\ Error) \newline<br>自定义 \newline<br>ce(Cross \ Entropy)<br>\end{cases}$</li>
</ul>
<h4 id="3-4-1-均方误差-MSE"><a href="#3-4-1-均方误差-MSE" class="headerlink" title="3.4.1 均方误差 MSE"></a>3.4.1 均方误差 MSE</h4><ul>
<li>$MSE(y_{-},y) = \frac{\sum^n_{k=0}(y-y_{-})^2}{n}$，即前向传播计算出的结果 $y$ 与已知答案 $y_{-}$ 之差的平方，再求平均</li>
<li><code>loss_mse = tf.reduce_mean(tf.square(y_-y))</code></li>
</ul>
<p>示例：</p>
<blockquote>
<p>预测酸奶日产量 $y$ ，$x1、x2$ 是影响日销量的因素</p>
</blockquote>
<p>建模前，应预先采集的数据集有：每日 $x1、x2$ 和实际销量 $y_{-}$ </p>
<p>此处没有提供数数据集，所以我们通过自行构造数据进行训练。随机生成 $x1、x2$  的数值，使 $y_{-}=x1+x2$ ，同时，为了使数据更真实，给求和后的 $y_{-}$ 引入随机噪声 $-0.05$ - $+0.05$ </p>
<p>把这套自制数据集喂入神经网络，构建一个一层的神经网络，预测酸奶日销量</p>
<p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf
import numpy as np

SEED = 23455

rdm = np.random.RandomState(seed = SEED) # 生成 [0, 1) 的随机数

# 构造数据
x = rdm.rand(32, 2)
y_ = [[x1 + x2 + (rdm.rand() / 10.0 - 0.05)] for (x1, x2) in x]

x = tf.cast(x, dtype = tf.float32)

w1 = tf.Variable(tf.random.normal([2, 1], stddev = 1, seed = 1))

epoch = 15000
lr = 0.002

for epoch in range(epoch):
    with tf.GradientTape() as tape:
        y = tf.matmul(x, w1)
        loss_mse = tf.reduce_mean(tf.square(y_ - y))
        
    grads = tape.gradient(loss_mse, w1)
    w1.assign_sub(lr * grads)
    
    if epoch % 500 == 0:
        print("After %d training steps, w1 is " % (epoch))
        print(w1.numpy(), "\n")

print("Final w1 is: ", w1.numpy())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="3-4-2-自定义损失函数"><a href="#3-4-2-自定义损失函数" class="headerlink" title="3.4.2 自定义损失函数"></a>3.4.2 自定义损失函数</h4><blockquote>
<p>预测酸奶日产量 $y$ ，$x1、x2$ 是影响日销量的因素</p>
</blockquote>
<p>对于该问题，前一节使用均方误差作为损失函数，默认认为销量预测多了或少了，产生的损失是一样的。然而真实情况是，预测多了，损失成本，预测少了，损失利润，$利润 \neq 成本$。在这种情况下，使用均方误差 $MSE$ 计算 $loss$ 是无法使利益最大化的。</p>
<p>此时，可以使用自定义损失函数：$loss(y_{-},y)=\sum f(y_{-},y)$，使用每一个预测结果 $y$ 与实际值 $y_{-}$ 产生的损失累积和，作为 $loss$</p>
<p>此题，可以把损失函数 $loss$ 定义为分段函数：$f(y_{-},y)=\begin{cases}<br>PROFIT*(y_{-} - y) &amp; y&lt;y_{-} \newline<br>COST*(y - y_{-}) &amp; y&gt;=y_{-}<br>\end{cases}$​</p>
<ul>
<li><code>loss_zdy = tf.reduce_sum(tf.where(tf.greater(y,y_),COST*(y-y_),PROFIT*(y_-y)))</code></li>
</ul>
<blockquote>
<p>更新题目，预测酸奶销量，酸奶成本（COST）1元，酸奶利润（PROFIT）99元</p>
</blockquote>
<p>预测少了一件损失利润 $99$ 元，预测多了一件损失成本 $1$ 元。显然，预测少了损失更大，所以我们希望生成的预测函数往预测多了的方向去预测</p>
<p>代码如下：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf
import numpy as np

SEED = 23455

rdm = np.random.RandomState(seed = SEED) # 生成 [0, 1) 的随机数

# 构造数据
x = rdm.rand(32, 2)
y_ = [[x1 + x2 + (rdm.rand() / 10.0 - 0.05)] for (x1, x2) in x]

x = tf.cast(x, dtype = tf.float32)

w1 = tf.Variable(tf.random.normal([2, 1], stddev = 1, seed = 1))

epoch = 10000
lr = 0.002

COST = 1
PROFIT = 99

for epoch in range(epoch):
    with tf.GradientTape() as tape:
        y = tf.matmul(x, w1)
        loss_zdy = tf.reduce_sum(tf.where(tf.greater(y,y_),COST*(y-y_),PROFIT*(y_-y)))
        
    grads = tape.gradient(loss_zdy, w1)
    w1.assign_sub(lr * grads)
    
    if epoch % 500 == 0:
        print("After %d training steps, w1 is " % (epoch))
        print(w1.numpy(), "\n")

print("Final w1 is: ", w1.numpy())<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<h4 id="3-4-3-交叉熵损失函数-CE"><a href="#3-4-3-交叉熵损失函数-CE" class="headerlink" title="3.4.3 交叉熵损失函数 CE"></a>3.4.3 交叉熵损失函数 CE</h4><ul>
<li>交叉熵损失函数 $CE\ (Cross Entropy)$ ：表征两个概率分布之间的距离，交叉熵越小，两个概率分布越近，交叉熵越大，两个概率分布越远</li>
<li>$H(y_{-},y)=-\sum (y_{-}*ln y)$​</li>
<li><code>tf.losses.categorical_crossentropy(y_,y)</code></li>
</ul>
<p><img src="/2024/02/06/tensorflow-2-0/image-20240222170501508.png"></p>
<p>使用代码验证计算结果：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf

loss_ce1 = tf.losses.categorical_crossentropy([1,0],[0.6,0.4])
loss_ce2 = tf.losses.categorical_crossentropy([1,0],[0.8,0.2])

print("loss_ce1: ",loss_ce1)
print("loss_ce2: ",loss_ce2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">loss_ce1:  tf.Tensor(0.5108256, shape=(), dtype=float32)
loss_ce2:  tf.Tensor(0.22314353, shape=(), dtype=float32)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</blockquote>
<h4 id="3-4-4-softmax-与-交叉熵-结合"><a href="#3-4-4-softmax-与-交叉熵-结合" class="headerlink" title="3.4.4 softmax 与 交叉熵 结合"></a>3.4.4 softmax 与 交叉熵 结合</h4><ul>
<li>输出先经过 $softmax()$ ，让输出结果符合概率分布</li>
<li>再求 $y_{-}$ 与 $y$ 的交叉熵损失函数</li>
<li><code>tf.nn.softmax_cross_entropy_with_logits(y_,y)</code>，同时计算概率分布和交叉熵</li>
</ul>
<p>举例：</p>
<pre class="line-numbers language-python" data-language="python"><code class="language-python">import tensorflow as tf
import numpy as np

y_ = np.array([[1,0,0],[0,1,0],[0,0,1],[1,0,0],[0,1,0]])
y = np.array([[12,3,2],[3,10,1],[1,2,5].[4,6.5,1.2],[3,6,1]])

y_pro = tf.nn.softmax(y)

loss_ce1 = tf.losses.categorical_crossentropy(y_,y_pro)
loss_ce2 = tf.nn.softmax_cross_entropy_with_logits(y_,y)

print("分布计算的结果：",loss_ce1)
print("结合计算的结果：",loss_ce2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>

<p>运行结果：</p>
<blockquote>
<pre class="line-numbers language-none"><code class="language-none">分布计算的结果： tf.Tensor(
[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00
 5.49852354e-02], shape=(5,), dtype=float64)
 
结合计算的结果： tf.Tensor(
[1.68795487e-04 1.03475622e-03 6.58839038e-02 2.58349207e+00
 5.49852354e-02], shape=(5,), dtype=float64)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</blockquote>
<h3 id="3-5-拟合效果"><a href="#3-5-拟合效果" class="headerlink" title="3.5 拟合效果"></a>3.5 拟合效果</h3><p><img src="/2024/02/06/tensorflow-2-0/image-20240223153137866.png"></p>
<ul>
<li><p>欠拟合：对现有数据集拟合不彻底，不能有效表征各数据点</p>
<ul>
<li><strong>增加输入特征项</strong>，给网络更多维度得输入特征</li>
<li><strong>增加网络参数</strong>，扩展网络规模，增加网络深度，提升模型表达力</li>
<li>减少正则化参数</li>
</ul>
</li>
<li><p>过拟合：对当前数据拟合得太好了，但对新数据难以做出正确得判断和预测，模型缺乏泛化力</p>
<ul>
<li><strong>数据清洗</strong>，减少噪声，使数据更纯净</li>
<li><strong>增大训练集</strong>，让模型见到更多的数据</li>
<li><strong>采用正则化</strong></li>
<li>增大正则化参数</li>
</ul>
</li>
</ul>
<h4 id="3-5-1-正则化缓解过拟合"><a href="#3-5-1-正则化缓解过拟合" class="headerlink" title="3.5.1 正则化缓解过拟合"></a>3.5.1 正则化缓解过拟合</h4><ul>
<li>正则化：在损失函数中引入模型复杂度指标，给每个参数 $w$ 加上权重，抑制训练集中数据的噪声。正则化通常只对参数 $w$ 使用，一般不对偏置 $b$ 使用正则化</li>
<li>$loss = loss(y,y_{-})+REGULARIZER*loss(w)$<ul>
<li>$loss(y,y_{-})$ ：前向传播预测结果 $y$ 与实际值 $y_{-}$ 的差距，即损失函数，如：交叉熵、均方误差</li>
<li>$REGULARIZER$ ：超参数，给出参数 $w$ 在总 $loss$ 中的比例，即正则化权重</li>
<li>$loss(w)$ ：需要正则化的参数，有两种计算方式$\begin{cases} loss_{L1}(w)=\sum|w_i| \newline loss_{L2}(w)=\sum|w_i^2| \end{cases}$ <ul>
<li>$L1$ 正则化大概率会使很多参数变为 $0$ ，因此该方法可通过稀疏参数，即减少参数的数量，降低模型复杂度</li>
<li>$L2$ 正则化会使参数很接近 $0$ 但不为 $0$ ，因此该方法可通过减少参数值的大小，可以有效缓解数据集中因噪声引起的过拟合，降低模型复杂度</li>
</ul>
</li>
</ul>
</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">阿淞啊</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://yijingsong.github.io/2024/02/06/tensorflow-2-0/">https://yijingsong.github.io/2024/02/06/tensorflow-2-0/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">阿淞啊</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            <span class="chip bg-color">无标签</span>
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2024/04/01/openmv/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/5.jpg" class="responsive-img" alt="OpenMv Cam H7">
                        
                        <span class="card-title">OpenMv Cam H7</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2024-04-01
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            阿淞啊
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/01/13/shang-wei-ji/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/16.jpg" class="responsive-img" alt="c#上位机开发">
                        
                        <span class="card-title">c#上位机开发</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-01-13
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            阿淞啊
                            
                        </span>
                    </div>
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>


  <!-- 是否加载使用自带的 prismjs. -->
  <script type="text/javascript" src="/libs/prism/prism.min.js"></script>


<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>



<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023-2024</span>
            
            <a href="/about" target="_blank">阿淞啊</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            
            <br>
            
                &nbsp;<i class="fas fa-chart-area"></i>&nbsp;站点总字数:&nbsp;<span
                        class="white-color">40.4k</span>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2023";
                        var startMonth = "1";
                        var startDate = "14";
                        var startHour = "22";
                        var startMinute = "20";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/yijingsong" class="tooltipped" target="_blank" data-tooltip="我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1633684763" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1633684763" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>



    <a href="mailto:1633684763@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我: 1633684763@qq.com" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>



    <a href="https://blog.csdn.net/qq_46372372" class="tooltipped" target="_blank" data-tooltip="CSDN博客: https://blog.csdn.net/qq_46372372" data-position="top" data-delay="50">
        <i class="fab fa-csdn">C</i>
    </a>



</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
    
    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    
    
    <script type="text/javascript">
        //只在桌面版网页启用特效
        var windowWidth = $(window).width();
        if (windowWidth > 768) {
            document.write('<script type="text/javascript" src="/js/sakura.js"><\/script>');
        }
        </script>
                

</body>

</html>
